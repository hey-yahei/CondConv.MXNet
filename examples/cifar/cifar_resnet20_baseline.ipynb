{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MXNet package\n",
    "import mxnet as mx\n",
    "from mxnet import nd, init, cpu, gpu, gluon, autograd\n",
    "# from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon.data.vision import CIFAR10, transforms as T\n",
    "from gluoncv.data import transforms as gcv_T\n",
    "from gluoncv.model_zoo import cifar_resnet20_v1\n",
    "\n",
    "# Normal package\n",
    "import time\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    # Model\n",
    "    num_class = 10\n",
    "    \n",
    "    # Train\n",
    "    max_steps = 80000\n",
    "    train_batch_size = 128\n",
    "    val_batch_size = 256\n",
    "    train_num_workers = 4\n",
    "    val_num_workers = 4\n",
    "    lr = 0.1\n",
    "    wd = 0.0001\n",
    "    momentum = 0.9\n",
    "    lr_decay = [(40000, 0.1), (60000, 0.1)]\n",
    "    \n",
    "    # Record\n",
    "    ckpt_dir = \"./tmp/checkpoints\"\n",
    "    main_tag = 'baseline'\n",
    "    ckpt_prefix = 'baseline'\n",
    "    train_record_per_steps = 200\n",
    "    val_per_steps = 400\n",
    "    spotter_starts_at = 10000\n",
    "    spotter_window_size = 10\n",
    "    patience = 20\n",
    "    snapshot_per_steps = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(Config.ckpt_dir):\n",
    "    os.makedirs(Config.ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_stamp = time.strftime('%Y%m%d_%H%M%S',time.localtime(time.time()))\n",
    "writer = SummaryWriter(log_dir=\"tmp/runs/{}_{}\".format(Config.main_tag, datetime_stamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cifar_resnet20_v1()\n",
    "net.initialize()\n",
    "net.collect_params().reset_ctx(gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, num_class, loss_func, dataloader, ctx):\n",
    "    t = time.time()\n",
    "    test_num_correct = 0\n",
    "    eval_loss = 0.\n",
    "\n",
    "    actives = 0\n",
    "    for X, y in dataloader:\n",
    "        X = X.as_in_context(ctx)\n",
    "        y = y.as_in_context(ctx)\n",
    "\n",
    "        outputs = net(X)\n",
    "        loss = loss_func(outputs, y)\n",
    "        eval_loss += loss.sum().asscalar()\n",
    "        pred = outputs.argmax(axis=1)\n",
    "        test_num_correct += (pred == y.astype('float32')).sum().asscalar()\n",
    "\n",
    "    eval_loss /= len(test_dataset)\n",
    "    eval_acc = test_num_correct / len(test_dataset)\n",
    "    return eval_loss, eval_acc, time.time()-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer = T.Compose([\n",
    "    gcv_T.RandomCrop(32, pad=4),\n",
    "    T.RandomFlipLeftRight(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "eval_transformer = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10(train=True).transform_first(train_transformer)\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=Config.train_batch_size,\n",
    "                          num_workers=Config.train_num_workers,\n",
    "                          last_batch='discard')\n",
    "test_dataset = CIFAR10(train=False).transform_first(eval_transformer)\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=Config.val_batch_size, \n",
    "                         shuffle=False,\n",
    "                         num_workers=Config.val_num_workers, \n",
    "                         last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "train_size = len(train_dataset)\n",
    "val_size = len(test_dataset)\n",
    "print(f'trainset size => {train_size}')\n",
    "print(f'valset size => {val_size}')\n",
    "steps_per_epoch = train_size / Config.train_batch_size\n",
    "print(f'{steps_per_epoch} steps for per epoch (BATCH_SIZE={Config.train_batch_size})')\n",
    "print(\"record per {} steps ({} samples, {} times per epoch)\".format(\n",
    "                                                            Config.train_record_per_steps,\n",
    "                                                            Config.train_record_per_steps * Config.train_batch_size,\n",
    "                                                            steps_per_epoch / Config.train_record_per_steps))\n",
    "print(\"evaluate per {} steps ({} times per epoch)\".format(\n",
    "                                                    Config.val_per_steps,\n",
    "                                                    steps_per_epoch / Config.val_per_steps))\n",
    "print(\"spotter start at {} steps ({} epoches)\".format(\n",
    "                                                Config.spotter_starts_at,\n",
    "                                                Config.spotter_starts_at / steps_per_epoch))\n",
    "print(\"size of spotter window is {} ({} steps)\".format(\n",
    "                                                Config.spotter_window_size,\n",
    "                                                Config.spotter_window_size * Config.val_per_steps))\n",
    "print(\"max patience: {} ({} steps; {} samples; {} epoches)\".format(\n",
    "                                                            Config.patience,\n",
    "                                                            Config.patience * Config.val_per_steps,\n",
    "                                                            Config.patience * Config.val_per_steps * Config.train_batch_size,\n",
    "                                                            Config.patience * Config.val_per_steps / steps_per_epoch))\n",
    "print(\"snapshot per {} steps ({} times per epoch)\".format(\n",
    "                                                    Config.snapshot_per_steps,\n",
    "                                                    steps_per_epoch / Config.snapshot_per_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_steps = 0\n",
    "good_acc_window = [0.] * Config.spotter_window_size\n",
    "estop_loss_window = [0.] * Config.patience\n",
    "loss_func = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'nag', \n",
    "                        {'learning_rate': Config.lr, 'wd': Config.wd, 'momentum': Config.momentum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_per_record = Config.train_record_per_steps * Config.train_batch_size\n",
    "flag_early_stop = False\n",
    "train_loss = 0.\n",
    "train_num_correct = 0\n",
    "t = time.time()\n",
    "\n",
    "while global_steps < Config.max_steps and not flag_early_stop:\n",
    "    for X, y in train_loader:\n",
    "        global_steps += 1\n",
    "        # Move data to gpu\n",
    "        X = X.as_in_context(gpu(0))\n",
    "        y = y.as_in_context(gpu(0))\n",
    "        \n",
    "        # Forward & Backward\n",
    "        with autograd.record():\n",
    "            outputs = net(X)\n",
    "            loss = loss_func(outputs, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        trainer.step(Config.train_batch_size)\n",
    "        \n",
    "        train_loss += loss.sum().asscalar()\n",
    "        pred = outputs.argmax(axis=1)\n",
    "        train_num_correct += (pred == y.astype('float32')).sum().asscalar()\n",
    "        \n",
    "        # Record training info\n",
    "        if global_steps and global_steps % Config.train_record_per_steps == 0:\n",
    "            writer.add_scalars(f'{Config.main_tag}/Loss', {'train': train_loss/size_per_record}, global_steps)\n",
    "            writer.add_scalars(f'{Config.main_tag}/Acc', {'train': train_num_correct/size_per_record}, global_steps)\n",
    "            train_loss = 0.\n",
    "            train_num_correct = 0\n",
    "            \n",
    "        # Evaluate\n",
    "        if global_steps and global_steps % Config.val_per_steps == 0:\n",
    "            # Evaluate\n",
    "            eval_loss, eval_acc, __ = evaluate(net, Config.num_class, loss_func, test_loader, ctx=gpu(0))\n",
    "            writer.add_scalar(f'{Config.main_tag}/Speed', Config.val_per_steps / (time.time() - t), global_steps)\n",
    "            writer.add_scalars(f'{Config.main_tag}/Loss', {'val': eval_loss}, global_steps)\n",
    "            writer.add_scalars(f'{Config.main_tag}/Acc', {'val': eval_acc}, global_steps)\n",
    "            \n",
    "#             # Spotter\n",
    "#             good_acc_window.pop(0)\n",
    "#             if global_steps >= Config.spotter_starts_at and eval_acc > max(good_acc_window):\n",
    "#                 print( \"catch a good model with acc {:.6f} at {} step\".format(eval_acc, global_steps) )\n",
    "#                 writer.add_text(Config.main_tag, \"catch a good model with acc {:.6f}\".format(eval_acc), global_steps)\n",
    "#                 net.save_parameters(\"{}/{}-{:06d}.params\".format(Config.ckpt_dir, Config.ckpt_prefix, global_steps))\n",
    "#             good_acc_window.append(eval_acc)\n",
    "\n",
    "#             # Early stop\n",
    "#             estop_loss_window.pop(0)\n",
    "#             estop_loss_window.append(eval_loss)\n",
    "#             if global_steps > Config.val_per_steps * len(estop_loss_window):\n",
    "#                 min_index = estop_loss_window.index( min(estop_loss_window) )\n",
    "#                 writer.add_scalar(f'{Config.main_tag}/val/Patience', min_index, global_steps)\n",
    "#                 if min_index == 0:\n",
    "#                     flag_early_stop = True\n",
    "#                     print(\"early stop at {} steps\".format(global_steps))\n",
    "#                     break\n",
    "            \n",
    "            t = time.time()\n",
    "        \n",
    "            # lr decay\n",
    "            if Config.lr_decay and Config.lr_decay[0][0] <= global_steps:\n",
    "                new_lr = trainer.learning_rate * Config.lr_decay.pop(0)[1]\n",
    "                print(f\"LR Decay: {trainer.learning_rate} -> {new_lr}\")\n",
    "                trainer.set_learning_rate(new_lr)\n",
    "        \n",
    "#         # Snapshot\n",
    "#         if global_steps and global_steps % Config.snapshot_per_steps == 0:\n",
    "#             net.save_parameters(\"{}/{}-{:06d}.params\".format(Config.ckpt_dir, Config.ckpt_prefix, global_steps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
